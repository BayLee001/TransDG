# -*- coding: utf-8 -*-
import os
import json
import codecs
import pickle
import numpy as np
from .schema import Schema
from ..utils.link_data import LinkData
from ..utils.log_util import LogInfo

schema_level_dict = {'strict': 0, 'elegant': 1, 'coherent': 2, 'general': 3}
train_pos = 75910
valid_pos = 86755


class SchemaDataset:

    def __init__(self, data_dir, candgen_dir, schema_level, freebase_helper,
                 path_max_size=3, file_list_name='all_list', full_constr=False):

        self.data_dir = data_dir
        self.candgen_dir = candgen_dir
        self.schema_level = schema_level
        self.freebase_helper = freebase_helper

        self.path_max_size = path_max_size
        self.file_list_name = file_list_name
        self.full_constr = full_constr

        self.q_idx_fp = "%s/q_idx.pkl" % self.candgen_dir
        self.spt_q_idx_fp = "%s/spt_q_idx.pkl" % self.candgen_dir
        self.q_cand_fp = "%s/q_cand.pkl" % self.candgen_dir
        self.active_dicts_fp = "%s/active_dicts.pkl" % self.candgen_dir
        self.active_uhashs_fp = "%s/active_uhashs.pkl" % self.candgen_dir

        # Need to save: Tvt split information & detail schema information
        self.q_idx_list = []          # indicating all active questions
        self.spt_q_idx_lists = []     # T/v/t list
        self.qa_list = []
        self.smart_q_cand_dict = {}
        self.active_dicts = {'word': {}, 'mid': {}, 'path': {}}
        self.active_uhashs = {'word': [], 'mid': [], 'path': []}

    def load_simpq(self):
        LogInfo.logs('Loading SimpleQuestions from pickle ...')
        pickle_fp = '%s/simpQ.data.pkl' % self.data_dir
        with open(pickle_fp, 'rb') as br:
            self.qa_list = pickle.load(br)
        LogInfo.logs('%d SimpleQuestions loaded.' % len(self.qa_list))

    def load_all_data(self):
        if len(self.smart_q_cand_dict) > 0:         # already loaded
            return
        # Load qa_list
        self.load_simpq()

        # Load schema information
        if not os.path.isfile(self.q_idx_fp):      # no dump, read from txt
            self.load_simpq_schemas_from_txt()

        else:
            self.load_schemas_from_pickle()

        LogInfo.begin_track('Meta statistics:')
        LogInfo.logs('Total questions = %d', len(self.q_idx_list))
        LogInfo.logs('T / v / t questions = %s', [len(lst) for lst in self.spt_q_idx_lists])
        LogInfo.logs('Active Word / Mid / Path = %d / %d / %d (with PAD, START, UNK)',
                     len(self.active_dicts['word']),
                     len(self.active_dicts['mid']),
                     len(self.active_dicts['path']))
        cand_size_dist = np.array([len(v) for v in self.smart_q_cand_dict.values()])
        LogInfo.logs('Total schemas = %d, avg = %.6f.', np.sum(cand_size_dist), np.mean(cand_size_dist))
        qlen_dist = np.array([len(qa['tokens']) for qa in self.qa_list])
        LogInfo.logs('Avg question length = %.6f.', np.mean(qlen_dist))
        LogInfo.end_track()

    def load_schema_kq(self, q_idx, schema_fp, gather_linkings,
                       sc_len_dist, path_len_dist, ans_size_dist):
        """
        Read the schema files generated by KQ.
        """
        schema_level = schema_level_dict[self.schema_level]

        candidate_list = []
        with codecs.open(schema_fp, 'r', 'utf-8') as br:
            sc_lines = br.readlines()
            for ori_idx, sc_line in enumerate(sc_lines):
                schema = Schema()
                # load schemas
                schema.read_schema_from_json(q_idx=q_idx, json_line=sc_line, gather_linkings=gather_linkings,
                                             ori_idx=ori_idx, full_constr=self.full_constr)
                # create the path_list on-the-fly
                schema.construct_path_list()

                real_sc_len = len(schema.path_list)
                sc_len_dist.append(len(schema.path_list))
                ans_size_dist.append(schema.ans_size)

                for raw_path, path in zip(schema.raw_paths, schema.path_list):
                    path_len_dist.append(len(path))

                if real_sc_len <= self.path_max_size and self._schema_classification(schema) <= schema_level:
                    candidate_list.append(schema)

        return candidate_list, len(sc_lines)

    def _schema_classification(self, sc):
        for category, focus, pred_seq in sc.raw_paths:
            if category != 'Main':
                continue  # only consider main path
            if len(pred_seq) == 1:
                return 0
            elif self.freebase_helper.is_mediator_as_expect(pred=pred_seq[0]):
                return 0
            else:
                p1_range = self.freebase_helper.get_range(pred_seq[0])
                p2_domain = self.freebase_helper.get_domain(pred_seq[1])
                if p1_range == p2_domain:
                    return 1
                elif self.freebase_helper.is_type_contained_by(p1_range, p2_domain):
                    return 2
                else:
                    return 3
        return 3

    def load_schemas_from_pickle(self):
        LogInfo.begin_track('Loading smart_candidates from [%s] ...', self.candgen_dir)
        with open(self.q_idx_fp, 'rb') as br:
            self.q_idx_list = pickle.load(br)
        with open(self.spt_q_idx_fp, 'rb') as br:
            self.spt_q_idx_lists = pickle.load(br)
        with open(self.q_cand_fp, 'rb') as br:
            self.smart_q_cand_dict = pickle.load(br)
        with open(self.active_dicts_fp, 'rb') as br:
            self.active_dicts = pickle.load(br)
        with open(self.active_uhashs_fp, 'rb') as br:
            self.active_uhashs = pickle.load(br)
        LogInfo.end_track()

    def save_smart_cands(self):
        LogInfo.begin_track('Saving candidates into [%s] ...', self.candgen_dir)
        with open(self.q_idx_fp, 'wb') as bw:
            pickle.dump(self.q_idx_list, bw)
        with open(self.spt_q_idx_fp, 'wb') as bw:
            pickle.dump(self.spt_q_idx_lists, bw)
        with open(self.q_cand_fp, 'wb') as bw:
            pickle.dump(self.smart_q_cand_dict, bw)
        with open(self.active_dicts_fp, 'wb') as bw:
            pickle.dump(self.active_dicts, bw)
        with open(self.active_uhashs_fp, 'wb') as bw:
            pickle.dump(self.active_uhashs, bw)
        LogInfo.end_track()

    def load_simpq_schemas_from_txt(self):
        LogInfo.begin_track('Loading SimpQ schemas from [%s] ...', self.candgen_dir)

        # Step 1: Load Auxiliary Information
        list_fp = '%s/%s' % (self.candgen_dir, self.file_list_name)
        with open(list_fp, 'r') as br:
            schema_fp_list = list(map(lambda line: '%s/%s' % (self.candgen_dir, line.strip()), br.readlines()))
        LogInfo.logs('%d schema files found in [%s].', len(schema_fp_list), self.file_list_name)

        # Step 2: Traverse & Make Statistics
        sc_len_dist = []  # distribution of number of paths in a schema
        path_len_dist = []  # distribution of length of each path
        ans_size_dist = []  # distribution of answer size
        total_cand_size = useful_cand_size = 0

        for scan_idx, schema_fp in enumerate(schema_fp_list):
            if scan_idx % 1000 == 0:
                LogInfo.logs('%d / %d scanned.', scan_idx, len(schema_fp_list))
            link_fp = schema_fp[0: schema_fp.rfind('_')] + '_links'
            q_idx = int(schema_fp.split('/')[-1].split('_')[0])
            self.q_idx_list.append(q_idx)

            gather_linkings = []
            with codecs.open(link_fp, 'r', 'utf-8') as br:
                for gl_line in br.readlines():
                    tup_list = json.loads(gl_line.strip())
                    ld_dict = {k: v for k, v in tup_list}
                    gather_linkings.append(LinkData(**ld_dict))

            candidate_list, total_lines = self.load_schema_kq(
                q_idx=q_idx, schema_fp=schema_fp, gather_linkings=gather_linkings,
                sc_len_dist=sc_len_dist, path_len_dist=path_len_dist, ans_size_dist=ans_size_dist
            )
            total_cand_size += total_lines
            useful_cand_size += len(candidate_list)
            self.smart_q_cand_dict[q_idx] = candidate_list

        # T/v/t split
        self.q_idx_list.sort()

        self.spt_q_idx_lists.append(list(filter(lambda x: x < train_pos, self.q_idx_list)))
        self.spt_q_idx_lists.append(list(filter(lambda x: train_pos <= x < valid_pos, self.q_idx_list)))
        self.spt_q_idx_lists.append(list(filter(lambda x: x >= valid_pos, self.q_idx_list)))

        # Step 3: Show Statistics
        q_size = len(self.smart_q_cand_dict)
        q_len_dist = [len(qa['tokens']) for qa in self.qa_list]
        LogInfo.begin_track('[STAT] %d questions scanned:', q_size)
        LogInfo.logs('Total schemas = %d', total_cand_size)
        LogInfo.logs('Useful schemas = %d (%.3f%%)', useful_cand_size, 100. * useful_cand_size / total_cand_size)
        LogInfo.logs('Avg candidates = %.3f', 1. * useful_cand_size / q_size)
        cand_size_dist = [len(v) for v in self.smart_q_cand_dict.values()]
        for show_name, show_dist in zip(['q_len', 'cand_size', 'sc_len', 'path_len', 'ans_size'],
                                        [q_len_dist, cand_size_dist, sc_len_dist, path_len_dist, ans_size_dist]):
            dist_arr = np.array(show_dist)
            LogInfo.begin_track('Show %s distribution:', show_name)
            LogInfo.logs('Average: %.6f', np.mean(dist_arr))
            LogInfo.end_track()
        LogInfo.end_track()

        # Build path word vocabulary
        LogInfo.begin_track('Building active word / mid / path vocabulary ... ')
        self.build_active_voc()
        LogInfo.end_track()

        # Save the candidates
        self.save_smart_cands()

    def add_item_into_active_voc(self, category, value):
        if value not in self.active_dicts[category]:
            self.active_dicts[category][value] = len(self.active_dicts[category])
            self.active_uhashs[category].append(value)

    def build_active_voc(self):
        for category in ('word', 'mid', 'path'):
            for mask in ('<PAD>', '<START>', '<UNK>'):
                self.add_item_into_active_voc(category=category, value=mask)
        for mask in ('<E>', '<T>', '<Tm>', '<Ord>'):
            self.add_item_into_active_voc(category='word', value=mask)

        # Try collect word from all data, while mid / path from train only
        for mode, q_idx_list in zip(['train', 'valid', 'test'], self.spt_q_idx_lists):
            for q_idx in q_idx_list:
                # Scanning questions
                qa = self.qa_list[q_idx]
                lower_tok_list = [tok.lower() for tok in qa['tokens']]
                for tok in lower_tok_list:
                    self.add_item_into_active_voc(category='word', value=tok)
                for rel, head, dep in qa['parse']:
                    self.add_item_into_active_voc(category='word', value=rel)
                    self.add_item_into_active_voc(category='word', value='!' + rel)
                # Scanning schemas
                for cand in self.smart_q_cand_dict[q_idx]:
                    for raw_path, mid_seq in zip(cand.raw_paths, cand.path_list):
                        path_cate, gl_data, _ = raw_path
                        path_str = '%s|%s' % (path_cate, '\t'.join(mid_seq))
                        if mode == 'train':
                            self.add_item_into_active_voc(category='path', value=path_str)
                        for mid in mid_seq:
                            if mode == 'train':
                                self.add_item_into_active_voc(category='mid', value=mid)
                            p_name = self.freebase_helper.get_item_name(mid)
                            if p_name != '':
                                spt = p_name.split(' ')
                                for tok in spt:
                                    self.add_item_into_active_voc(category='word', value=tok)  # type / pred name

        for category in ('word', 'mid', 'path'):
            LogInfo.logs('Active %s size = %d', category, len(self.active_dicts[category]))
