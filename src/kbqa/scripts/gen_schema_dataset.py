# -*- coding: utf-8 -*-
import os
import json
import codecs
import pickle
import argparse
import numpy as np
from src.kbqa.dataset.schema import Schema
from src.kbqa.utils.link_data import LinkData


class DialogueSchemaDataset:

    def __init__(self, data_dir, candgen_dir, mode='train', file_list_name='all_list', full_constr=False):

        self.data_dir = data_dir
        self.candgen_dir = candgen_dir
        self.mode = mode
        self.file_list_name = file_list_name
        self.full_constr = full_constr

        self.save_dir = '%s/%s' % (candgen_dir, file_list_name)

        self.qa_list = []
        self.q_idx_list = []  # indicating all active questions
        self.smart_q_cand_dict = {}
        self.active_dicts = {'word': {}, 'mid': {}, 'path': {}}
        self.active_uhashs = {'word': [], 'mid': [], 'path': []}

    def load_all_data(self):
        if len(self.smart_q_cand_dict) > 0:  # already loaded
            return

        self.load_reddit(mode=self.mode)

        self.load_reddit_schemas_from_txt()

        self.build_active_voc()

        self.save_smart_cands()

        print('Meta statistics:')
        print('Total posts = %d' % len(self.q_idx_list))
        print('Active Word / Mid / Path = %d / %d / %d (with PAD, START, UNK)' % (
              len(self.active_dicts['word']),
              len(self.active_dicts['mid']),
              len(self.active_dicts['path'])))
        cand_size_dist = np.array([len(v) for v in self.smart_q_cand_dict.values()])
        print('Total schemas = %d, avg = %.3f' % (np.sum(cand_size_dist), np.mean(cand_size_dist)))
        qlen_dist = np.array([len(qa['tokens']) for qa in self.qa_list])
        print('Avg post length = %.3f' % np.mean(qlen_dist))

    def load_reddit(self, mode='train'):
        print('Loading Reddit dialogs from pickle ...')
        pickle_fp = '%s/Reddit.%s.pkl' % (self.data_dir, mode)
        with open(pickle_fp, 'rb') as br:
            self.qa_list = pickle.load(br)
        print('%d Reddit dialogs loaded.' % len(self.qa_list))

    def save_smart_cands(self):
        print('Saving candidates into [%s] ...'% self.candgen_dir)
        with open("%s/q_idx.pkl" % self.candgen_dir, 'wb') as bw:
            pickle.dump(self.q_idx_list, bw)
        with open("%s/q_cand.pkl" % self.candgen_dir, 'wb') as bw:
            pickle.dump(self.smart_q_cand_dict, bw)
        with open("%s/active_dicts.pkl" % self.candgen_dir, 'wb') as bw:
            pickle.dump(self.active_dicts, bw)
        with open("%s/active_uhashs.pkl" % self.candgen_dir, 'wb') as bw:
            pickle.dump(self.active_uhashs, bw)

    def load_reddit_schemas_from_txt(self):
        print('Loading Reddit schemas from [%s] ...'% self.candgen_dir)

        # Step 1: Load Auxiliary Information
        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)

        list_fp = '%s/%s' % (self.candgen_dir, self.file_list_name)
        with open(list_fp, 'r') as br:
            schema_fp_list = list(map(lambda line: '%s/%s' % (self.candgen_dir, line.strip()), br.readlines()))
        print('%d schema files found in [%s].' % (len(schema_fp_list), self.file_list_name))

        # Step 2: Traverse & Make Statistics
        total_cand_size = 0

        for scan_idx, schema_fp in enumerate(schema_fp_list):
            if scan_idx > 0 and scan_idx % 10000 == 0:
                print('%d / %d scanned.' % (scan_idx, len(schema_fp_list)))
            link_fp = schema_fp[0: schema_fp.rfind('_')] + '_links'
            q_idx = int(schema_fp.split('/')[-1].split('_')[0])
            self.q_idx_list.append(q_idx)

            gather_linkings = []
            with codecs.open(link_fp, 'r', 'utf-8') as br:
                for gl_line in br.readlines():
                    tup_list = json.loads(gl_line.strip())
                    ld_dict = {k: v for k, v in tup_list}
                    gather_linkings.append(LinkData(**ld_dict))

            candidate_list, total_lines = self.load_schema_kq(
                q_idx=q_idx, schema_fp=schema_fp, gather_linkings=gather_linkings
            )
            total_cand_size += total_lines
            self.smart_q_cand_dict[q_idx] = candidate_list

        # Step 3: Show Statistics
        q_size = len(self.smart_q_cand_dict)
        print('%d posts scanned:' % q_size)
        print('Total schemas = %d' % total_cand_size)

    def load_schema_kq(self, q_idx, schema_fp, gather_linkings):
        """
        Read the schema files generated by KQ.
        """
        candidate_list = []
        with codecs.open(schema_fp, 'r', 'utf-8') as br:
            sc_lines = br.readlines()
            for ori_idx, sc_line in enumerate(sc_lines):
                schema = Schema()
                # load schemas
                schema.read_schema_from_json(q_idx=q_idx, json_line=sc_line, gather_linkings=gather_linkings,
                                             ori_idx=ori_idx, full_constr=self.full_constr)

                candidate_list.append(schema)

        return candidate_list, len(sc_lines)

    def add_item_into_active_voc(self, category, value):
        if value not in self.active_dicts[category]:
            self.active_dicts[category][value] = len(self.active_dicts[category])
            self.active_uhashs[category].append(value)

    def build_active_voc(self):
        print('Building active word / mid / path vocabulary ... ')
        for category in ('word', 'mid', 'path'):
            for mask in ('<PAD>', '<START>', '<UNK>'):
                self.add_item_into_active_voc(category=category, value=mask)
        for mask in ('<E>', '<T>', '<Tm>', '<Ord>'):
            self.add_item_into_active_voc(category='word', value=mask)
        for q_idx in self.q_idx_list:
            qa = self.qa_list[q_idx]
            lower_tok_list = [tok.lower() for tok in qa['tokens']]
            for tok in lower_tok_list:
                self.add_item_into_active_voc(category='word', value=tok)
            for rel, head, dep in qa['parse']:
                self.add_item_into_active_voc(category='word', value=rel)
                self.add_item_into_active_voc(category='word', value='!' + rel)

        for category in ('word', 'mid', 'path'):
            print('Active %s size = %d' % (category, len(self.active_dicts[category])))


def main(args):
    schema_dataset = DialogueSchemaDataset(args.data_dir, args.candgen_dir, mode=args.mode)
    schema_dataset.load_all_data()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Reddit schema dataset generation")
    parser.add_argument('--data_dir', type=str, help="Reddit data directory")
    parser.add_argument('--candgen_dir', type=str, help="Reddit candidates directory")
    parser.add_argument('--mode', type=str, choices=['train', 'valid', 'test'])
    parsed_args = parser.parse_args()

    main(parsed_args)
